{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi = {s: i+1 for i, s in enumerate(sorted(set(''.join(words))))}\n",
    "stoi['.'] = 0\n",
    "itos = {s: i for i, s in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.32033497,  0.06176215],\n",
       "       [ 1.8183279 ,  0.91623193]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_global_generator(42)\n",
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "a = tf.Variable(tf.random.normal((2, 3)), name='a')\n",
    "a@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([182663, 3]), TensorShape([22684, 3]), TensorShape([22684]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def build_dataset(words: list[str]):\n",
    "    X, Y = [], []\n",
    "    context_size = 3\n",
    "    for w in words:\n",
    "        context = [0] * context_size\n",
    "        for ch in w + \".\":\n",
    "            X.append(context)\n",
    "            Y.append(stoi[ch])\n",
    "            # output = [itos[i] for i in context]\n",
    "            # print(f\"{''.join(output)} ----> {ch}\")\n",
    "            context = context[1:] + [stoi[ch]]\n",
    "    return tf.Variable(X), tf.Variable(Y)\n",
    "\n",
    "\n",
    "n1, n2 = int(0.8 * len(words)), int(0.9 * len(words))\n",
    "random.shuffle(words)\n",
    "\n",
    "X_train, Y_train = build_dataset(words=words[:n1])\n",
    "X_dev, Y_dev = build_dataset(words=words[n1:n2])\n",
    "X_test, Y_test = build_dataset(words=words[n2:])\n",
    "\n",
    "# X_train, Y_train = X_train.to(device=device), Y_train.to(device=device)\n",
    "# X_dev, Y_dev = X_dev.to(device=device), Y_dev.to(device=device)\n",
    "# X_test, Y_test = X_test.to(device=device), Y_test.to(device=device)\n",
    "\n",
    "X_train.shape, X_dev.shape, Y_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = tf.Variable(tf.random.uniform(shape=(27, 5)), trainable=True)\n",
    "W1 = tf.Variable(tf.random.uniform(shape=(15, 200)), trainable=True)\n",
    "b1 = tf.Variable(tf.random.uniform(shape=(200,)), trainable=True)\n",
    "W2 = tf.Variable(tf.random.uniform(shape=(200, 27)), trainable=True)\n",
    "b2 = tf.Variable(tf.random.uniform(shape=(27,)), trainable=True)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=float32, numpy=\n",
       "array([[0.41601348, 0.01093221, 0.46235168, 0.25209248, 0.91174185,\n",
       "        0.41601348, 0.01093221, 0.46235168, 0.25209248, 0.91174185,\n",
       "        0.41601348, 0.01093221, 0.46235168, 0.25209248, 0.91174185]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = tf.gather(C, X_train)\n",
    "tf.reshape(ll[0], shape=(-1, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8762>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tf.size(p) for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward(inputs):\n",
    "    emb = tf.gather(C, inputs)\n",
    "    h = tf.tanh(tf.reshape(emb, shape=(-1, 3*5)) @ W1+b1)\n",
    "    logits = h@W2+b2\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n"
     ]
    }
   ],
   "source": [
    "stepi, lossi, lossd = [], [], []\n",
    "ix = tf.random.uniform(shape=(32,), minval=0, maxval=26, dtype=tf.int32)\n",
    "# print(ix)\n",
    "print(tf.gather(X_train, ix).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(68238,) and logits.shape=(22746, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m cross_entropy(targets, logits)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     15\u001b[0m     logits_dev \u001b[38;5;241m=\u001b[39m forward(X_dev)\n\u001b[0;32m---> 16\u001b[0m     loss_dev \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_dev\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_train\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  dev loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_dev\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-tensorflow/lib/python3.9/site-packages/keras/losses.py:142\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m in_mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(y_pred)\n\u001b[1;32m    145\u001b[0m out_mask \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39mget_mask(losses)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-tensorflow/lib/python3.9/site-packages/keras/losses.py:268\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    261\u001b[0m     y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    262\u001b[0m         y_pred, y_true\n\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    265\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mag_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-tensorflow/lib/python3.9/site-packages/keras/losses.py:2078\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[1;32m   2029\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.metrics.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.sparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2034\u001b[0m     y_true, y_pred, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ignore_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2035\u001b[0m ):\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sparse categorical crossentropy loss.\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m \n\u001b[1;32m   2038\u001b[0m \u001b[38;5;124;03m    Standalone usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;124;03m      Sparse categorical crossentropy loss value.\u001b[39;00m\n\u001b[1;32m   2077\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_categorical_crossentropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-tensorflow/lib/python3.9/site-packages/keras/backend.py:5664\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis, ignore_class)\u001b[0m\n\u001b[1;32m   5660\u001b[0m         res \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msparse_softmax_cross_entropy_with_logits(\n\u001b[1;32m   5661\u001b[0m             labels\u001b[38;5;241m=\u001b[39mtarget, logits\u001b[38;5;241m=\u001b[39moutput\n\u001b[1;32m   5662\u001b[0m         )\n\u001b[1;32m   5663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5664\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_softmax_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\n\u001b[1;32m   5666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5669\u001b[0m     res_shape \u001b[38;5;241m=\u001b[39m cast(output_shape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(68238,) and logits.shape=(22746, 27)"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "n_iter = 2\n",
    "for i in range(n_iter):\n",
    "\n",
    "    # minibatch\n",
    "    ix = tf.random.uniform(shape=(32,), minval=0,\n",
    "                           maxval=X_train.shape[0], dtype=tf.int32)\n",
    "    inputs = tf.gather(X_train, ix)\n",
    "    targets = tf.gather(Y_train, ix)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = forward(inputs)\n",
    "        loss_train = cross_entropy(targets, logits).numpy()\n",
    "\n",
    "        logits_dev = forward(X_dev)\n",
    "        print(logits_dev.shape)\n",
    "        print(logits_dev.shape)\n",
    "        loss_dev = cross_entropy(X_dev, logits_dev).numpy()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\n",
    "            f\"step {i}  train loss: {loss_train:.4f}  dev loss: {loss_dev:.4f}\")\n",
    "\n",
    "    gradients = tape.gradient(loss_train, parameters)\n",
    "    lr = 0.1\n",
    "    for p, g in zip(parameters, gradients):\n",
    "        p = p - lr * g\n",
    "\n",
    "    stepi.append(i)\n",
    "    lossi.append(tf.math.log(loss_train).numpy())\n",
    "    lossd.append(tf.math.log(loss_dev).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
