{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e m\n",
      "e m m\n",
      "m m a\n",
      "m a .\n",
      ". o l\n",
      "o l i\n",
      "l i v\n",
      "i v i\n",
      "v i a\n",
      "i a .\n",
      ". a v\n",
      "a v a\n",
      "v a .\n"
     ]
    }
   ],
   "source": [
    "for word in words[:3]:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        print(ch1, ch2, ch3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'a': 1,\n",
       "  'b': 2,\n",
       "  'c': 3,\n",
       "  'd': 4,\n",
       "  'e': 5,\n",
       "  'f': 6,\n",
       "  'g': 7,\n",
       "  'h': 8,\n",
       "  'i': 9,\n",
       "  'j': 10,\n",
       "  'k': 11,\n",
       "  'l': 12,\n",
       "  'm': 13,\n",
       "  'n': 14,\n",
       "  'o': 15,\n",
       "  'p': 16,\n",
       "  'q': 17,\n",
       "  'r': 18,\n",
       "  's': 19,\n",
       "  't': 20,\n",
       "  'u': 21,\n",
       "  'v': 22,\n",
       "  'w': 23,\n",
       "  'x': 24,\n",
       "  'y': 25,\n",
       "  'z': 26,\n",
       "  '.': 0},\n",
       " {1: 'a',\n",
       "  2: 'b',\n",
       "  3: 'c',\n",
       "  4: 'd',\n",
       "  5: 'e',\n",
       "  6: 'f',\n",
       "  7: 'g',\n",
       "  8: 'h',\n",
       "  9: 'i',\n",
       "  10: 'j',\n",
       "  11: 'k',\n",
       "  12: 'l',\n",
       "  13: 'm',\n",
       "  14: 'n',\n",
       "  15: 'o',\n",
       "  16: 'p',\n",
       "  17: 'q',\n",
       "  18: 'r',\n",
       "  19: 's',\n",
       "  20: 't',\n",
       "  21: 'u',\n",
       "  22: 'v',\n",
       "  23: 'w',\n",
       "  24: 'x',\n",
       "  25: 'y',\n",
       "  26: 'z',\n",
       "  0: '.'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i, s in enumerate(characters)}\n",
    "stoi[\".\"] = 0\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0, 207, 190,  ...,  27, 173, 152],\n",
       "         [  0, 169,   0,  ...,   0,   4,   0],\n",
       "         ...,\n",
       "         [  0,  57,   0,  ...,   1,  17,  11],\n",
       "         [  0, 246,   0,  ...,   0,   0,   2],\n",
       "         [  0, 456,   0,  ...,   0,  91,   1]],\n",
       "\n",
       "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 40,   0,   5,  ...,   0,  20,  11],\n",
       "         [ 36,  28,  20,  ...,   0,  12,   0],\n",
       "         ...,\n",
       "         [ 11,   5,   0,  ...,  17,   6,   3],\n",
       "         [163, 389,  13,  ...,   0,  16,  40],\n",
       "         [ 38, 123,   0,  ...,   0,  12,  22]],\n",
       "\n",
       "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 46,   5,   5,  ...,   4,  31,   4],\n",
       "         [  1,   8,   0,  ...,   0,   9,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 55,   4,   1,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 10,   0,   2,  ...,   0,  10,   0],\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         ...,\n",
       "         [ 18,   3,   0,  ...,   0,   1,   0],\n",
       "         [  5,   4,   0,  ...,   0,   0,   0],\n",
       "         [  0,  16,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [716,  46,  10,  ...,   3,   6,  21],\n",
       "         [  2,   2,   0,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 23,   1,   0,  ...,   1,   0,   0],\n",
       "         [  1,  18,   0,  ...,   0,   0,   0],\n",
       "         [  2,  27,   0,  ...,   1,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 98,  14,  40,  ...,   3,  97,   3],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [  1,   0,   0,  ...,   0,   0,   0],\n",
       "         [ 34,  27,   0,  ...,   0,   0,   1],\n",
       "         [  4,  13,   0,  ...,   0,   7,   0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in words:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        idx3 = stoi[ch3]\n",
    "        N[idx1, idx2, idx3] += 1\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "P = (N+1).float()\n",
    "P /= P.sum(2, keepdim=True)\n",
    "P[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y', 'l', '.', 'm']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_idx = torch.multinomial(P[1, 1], 4, generator=g)\n",
    "selection = [itos[i.item()] for i in ex_idx]\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".wesmahle.\n",
      ".man.\n",
      ".roxgkthmang.\n",
      ".kyn.\n",
      ".layahd.\n",
      ".neen.\n",
      ".ana.\n",
      ".na.\n",
      ".brii.\n",
      ".yah.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = torch.randint(0, 27, (1,), generator=g).item()\n",
    "    out.extend([ix1, ix2])\n",
    "    while True:\n",
    "        index = torch.multinomial(\n",
    "            P[ix1, ix2], num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(index)\n",
    "        if index == 0:\n",
    "            break\n",
    "        ix1 = ix2\n",
    "        ix2 = index\n",
    "    # print(out)\n",
    "    name = [itos[i] for i in out]\n",
    "    print(''.join(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-410414.9688)\n",
      "196113\n",
      "tensor(2.0927)\n"
     ]
    }
   ],
   "source": [
    "# computiing the overall likelihood of the dataset\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for word in words:\n",
    "    chs = \".\" + word+\".\"\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        idx1, idx2, idx3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        log_prob = torch.log(P[idx1, idx2, idx3])\n",
    "        log_likelihood += log_prob\n",
    "        n += 1\n",
    "\n",
    "print(log_likelihood)\n",
    "print(n)\n",
    "print(-log_likelihood/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0002, 0.0469, 0.0430, 0.0072, 0.0827, 0.0126, 0.0050, 0.0041, 0.0207,\n",
       "        0.0349, 0.0063, 0.0171, 0.1427, 0.0868, 0.1406, 0.0025, 0.0041, 0.0023,\n",
       "        0.1089, 0.0439, 0.0165, 0.0345, 0.0550, 0.0016, 0.0063, 0.0392, 0.0345])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exemple of probabilities of next letters for the character 0(.) and 1(a)\n",
    "P[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([164080, 2]), torch.Size([164080]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for word in words:\n",
    "    chs = '.' + word + '.'\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[3:]):\n",
    "        idx1, idx2, idx3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        xs.append([idx1, idx2])\n",
    "        ys.append(idx3)\n",
    "\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)\n",
    "# num = ys.shape[0]\n",
    "W = torch.rand((27*2, 27), generator=g, requires_grad=True)\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0  loss : 3.3274776935577393\n",
      "step : 10  loss : 2.504183769226074\n",
      "step : 20  loss : 2.4609193801879883\n",
      "step : 30  loss : 2.4417195320129395\n",
      "step : 40  loss : 2.429647445678711\n",
      "step : 50  loss : 2.4207324981689453\n",
      "step : 60  loss : 2.413508415222168\n",
      "step : 70  loss : 2.407299757003784\n",
      "step : 80  loss : 2.401749610900879\n",
      "step : 90  loss : 2.3966519832611084\n",
      "step : 100  loss : 2.391878843307495\n",
      "step : 110  loss : 2.387345552444458\n",
      "step : 120  loss : 2.3829946517944336\n",
      "step : 130  loss : 2.3787851333618164\n",
      "step : 140  loss : 2.3746883869171143\n",
      "step : 150  loss : 2.3706822395324707\n",
      "step : 160  loss : 2.3667492866516113\n",
      "step : 170  loss : 2.3628768920898438\n",
      "step : 180  loss : 2.3590548038482666\n",
      "step : 190  loss : 2.3552756309509277\n",
      "step : 200  loss : 2.351532459259033\n",
      "step : 210  loss : 2.3478200435638428\n",
      "step : 220  loss : 2.3441343307495117\n",
      "step : 230  loss : 2.3404717445373535\n",
      "step : 240  loss : 2.336829423904419\n",
      "step : 250  loss : 2.333204746246338\n",
      "step : 260  loss : 2.3295950889587402\n",
      "step : 270  loss : 2.3259997367858887\n",
      "step : 280  loss : 2.3224170207977295\n",
      "step : 290  loss : 2.3188445568084717\n",
      "step : 300  loss : 2.3152825832366943\n",
      "step : 310  loss : 2.3117291927337646\n",
      "step : 320  loss : 2.3081841468811035\n",
      "step : 330  loss : 2.3046460151672363\n",
      "step : 340  loss : 2.301114797592163\n",
      "step : 350  loss : 2.2975895404815674\n",
      "step : 360  loss : 2.294069766998291\n",
      "step : 370  loss : 2.290555238723755\n",
      "step : 380  loss : 2.287045478820801\n",
      "step : 390  loss : 2.2835397720336914\n",
      "step : 400  loss : 2.280038595199585\n",
      "step : 410  loss : 2.276540517807007\n",
      "step : 420  loss : 2.2730460166931152\n",
      "step : 430  loss : 2.269554615020752\n",
      "step : 440  loss : 2.266066312789917\n",
      "step : 450  loss : 2.2625808715820312\n",
      "step : 460  loss : 2.2590978145599365\n",
      "step : 470  loss : 2.255617141723633\n",
      "step : 480  loss : 2.252138614654541\n",
      "step : 490  loss : 2.2486624717712402\n",
      "step : 500  loss : 2.245187997817993\n",
      "step : 510  loss : 2.241715669631958\n",
      "step : 520  loss : 2.2382452487945557\n",
      "step : 530  loss : 2.2347755432128906\n",
      "step : 540  loss : 2.2313084602355957\n",
      "step : 550  loss : 2.227842330932617\n",
      "step : 560  loss : 2.2243776321411133\n",
      "step : 570  loss : 2.220914125442505\n",
      "step : 580  loss : 2.2174525260925293\n",
      "step : 590  loss : 2.213991403579712\n",
      "step : 600  loss : 2.21053147315979\n",
      "step : 610  loss : 2.2070724964141846\n",
      "step : 620  loss : 2.2036149501800537\n",
      "step : 630  loss : 2.20015811920166\n",
      "step : 640  loss : 2.196702241897583\n",
      "step : 650  loss : 2.193247079849243\n",
      "step : 660  loss : 2.189793109893799\n",
      "step : 670  loss : 2.1863393783569336\n",
      "step : 680  loss : 2.182887077331543\n",
      "step : 690  loss : 2.1794350147247314\n",
      "step : 700  loss : 2.1759839057922363\n",
      "step : 710  loss : 2.1725332736968994\n",
      "step : 720  loss : 2.1690833568573\n",
      "step : 730  loss : 2.1656336784362793\n",
      "step : 740  loss : 2.162184953689575\n",
      "step : 750  loss : 2.1587367057800293\n",
      "step : 760  loss : 2.1552891731262207\n",
      "step : 770  loss : 2.1518421173095703\n",
      "step : 780  loss : 2.14839506149292\n",
      "step : 790  loss : 2.144949197769165\n",
      "step : 800  loss : 2.14150333404541\n",
      "step : 810  loss : 2.1380577087402344\n",
      "step : 820  loss : 2.134612798690796\n",
      "step : 830  loss : 2.1311686038970947\n",
      "step : 840  loss : 2.1277241706848145\n",
      "step : 850  loss : 2.1242804527282715\n",
      "step : 860  loss : 2.1208369731903076\n",
      "step : 870  loss : 2.117394208908081\n",
      "step : 880  loss : 2.1139514446258545\n",
      "step : 890  loss : 2.110508918762207\n",
      "step : 900  loss : 2.1070663928985596\n",
      "step : 910  loss : 2.1036248207092285\n",
      "step : 920  loss : 2.1001832485198975\n",
      "step : 930  loss : 2.0967421531677246\n",
      "step : 940  loss : 2.0933010578155518\n",
      "step : 950  loss : 2.089860200881958\n",
      "step : 960  loss : 2.0864200592041016\n",
      "step : 970  loss : 2.082979440689087\n",
      "step : 980  loss : 2.0795397758483887\n",
      "step : 990  loss : 2.0760998725891113\n",
      "step : 1000  loss : 2.072660446166992\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "n = 1000\n",
    "for i in range(n+1):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc.view(-1, 2*27) @ W\n",
    "    counts = torch.exp(logits)\n",
    "    # print(counts.shape)\n",
    "    probs = counts/counts.sum(dim=1, keepdim=True)\n",
    "    loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() - 0.1 * W.mean()\n",
    "\n",
    "    if i % 10 == 0 or i == 0 or i == n:\n",
    "        print(f\"step : {i}  loss : {loss.item()}\")\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".tli.\n",
      ".vtn.\n",
      "..yaansldn.\n",
      ".kil.\n",
      ".saiaadhn.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    ix0 = 0\n",
    "    ix1 = torch.randint(high=27, size=(1, 1), generator=g).item()\n",
    "    out = [ix0, ix1]\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor(\n",
    "            [out[ix0], out[ix0+1]]), num_classes=27).float()\n",
    "        logits = xenc.view(-1, 2*27)@W\n",
    "        counts = logits.exp()\n",
    "        probs = counts/counts.sum(dim=1, keepdim=True)\n",
    "        ix = torch.multinomial(probs, num_samples=1,\n",
    "                               replacement=True, generator=g).item()\n",
    "        out.append(ix)\n",
    "        if (ix == 0):\n",
    "            break\n",
    "        ix0 += 1\n",
    "    # print(itos(out))\n",
    "    print(''.join([itos[s] for s in out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Train, Test,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(words)\n",
    "num = len(words)\n",
    "train, dev, test = words[:int(\n",
    "    0.8*num)], words[int(0.8*num):int(0.9*num)], words[int(0.9*num):]\n",
    "len(train) + len(dev) + len(test) == num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in train:\n",
    "    chs = ['.'] + list(word) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        idx3 = stoi[ch3]\n",
    "        N[idx1, idx2, idx3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for word in train:\n",
    "    chs = '.' + word + '.'\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[3:]):\n",
    "        idx1, idx2, idx3 = stoi[ch1], stoi[ch2], stoi[ch3]\n",
    "        xs.append([idx1, idx2])\n",
    "        ys.append(idx3)\n",
    "\n",
    "\n",
    "xs, ys = torch.tensor(xs), torch.tensor(ys)\n",
    "# num = ys.shape[0]\n",
    "W = torch.rand((27*2, 27), generator=g, requires_grad=True)\n",
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "for i in range(n+1):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc.view(-1, 2*27) @ W\n",
    "    counts = torch.exp(logits)\n",
    "    # print(counts.shape)\n",
    "    probs = counts/counts.sum(dim=1, keepdim=True)\n",
    "    loss = -probs[torch.arange(ys.shape[0]), ys].log().mean() - 0.1 * W.mean()\n",
    "\n",
    "    if i % 100 == 0 or i == 0 or i == n:\n",
    "        print(f\"step : {i}  loss : {loss.item()}\")\n",
    "\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
